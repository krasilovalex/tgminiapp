
import requests
import time
import random
import os
import json
from ollama_api import analyze_prompt_with_ollama, query_ollama_for_feedback, OLLAMA_API_URL
from wikipedia_api import get_wikipedia_summary, get_wikipedia_article_for_llama
from data_handler import load_data, save_data, register_user, update_progress, update_test_results, load_tests, THEMES



CACHE_DIR = "cache"
CACHE_EXPIRATION_TIME = 3600   # 1 HOURS



# –§—É–Ω–∫—Ü–∏—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–µ–º—ã —Å –ø–æ–º–æ—â—å—é LLaMA
def extract_topic_from_prompt(prompt):
    system_prompt = (
        "–¢—ã ‚Äî –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –ø—Ä–æ–º–ø—Ç–æ–≤. –û–ø—Ä–µ–¥–µ–ª–∏ –æ—Å–Ω–æ–≤–Ω—É—é —Ç–µ–º—É —Å–ª–µ–¥—É—é—â–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞, "
        "–≤—ã–±—Ä–∞–≤ –æ–¥–Ω–æ –≥–ª–∞–≤–Ω–æ–µ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ –∏–ª–∏ —Ñ—Ä–∞–∑—É. –ù–µ –¥–æ–±–∞–≤–ª—è–π –ª–∏—à–Ω–µ–≥–æ —Ç–µ–∫—Å—Ç–∞, "
        "–ø—Ä–æ—Å—Ç–æ —É–∫–∞–∂–∏ —Ç–µ–º—É."
    )

    enriched_prompt = f"{system_prompt}\n\n–ü—Ä–æ–º–ø—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {prompt}"
    response = analyze_prompt_with_ollama(enriched_prompt)

    return response.strip()


# –§—É–Ω–∫—Ü–∏—è –≤—ã–±–æ—Ä–∞ —Ç–µ–º—ã(—Å–ª—É—á–∞–π–Ω–æ–π)
def get_next_theme(user_id):
    data = load_data()
    user_data = data["users"].get(str(user_id))

    if not user_data:
        return "–°–Ω–∞—á–∞–ª–æ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä—É–π—Ç–µ—Å—å —Å –ø–æ–º–æ—â—å—é /start!"
    
    completed_themes = user_data["progress"]["completed_themes"]

    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–µ–º—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –µ—â–µ –Ω–µ –∏–∑—É—á–∞–ª

    available_themes = [theme for theme in THEMES if theme not in completed_themes]

    if not available_themes:
        return "–í—ã –∏–∑—É—á–∏–ª–∏ –≤—Å–µ —Ç–µ–º—ã!üéâ –û–∂–∏–¥–∞–π—Ç–µ –Ω–æ–≤—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤."
    
    next_theme = random.choice(available_themes) # –í—ã–≤–æ–¥–∏–º —Å–ª—É—á–∞–π–Ω—É—é —Ç–µ–º—É

    return f"üìö *–¢–µ–º–∞ –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è:*\n‚û°{next_theme}"


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–æ–ø –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤
def get_additional_materials_for_topic_with_llama(topic, lang="ru"):
    # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –¥–ª—è LLaMA —Å —Ç–µ–º–æ–π
    prompt = f"–ü—Ä–µ–¥–æ—Å—Ç–∞–≤—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã –ø–æ —Ç–µ–º–µ: {topic}. –í–∫–ª—é—á–∏ –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ, –∫–ª—é—á–µ–≤—ã–µ –∞—Å–ø–µ–∫—Ç—ã, —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ä–µ—Å—É—Ä—Å—ã, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å, –∏ –ø—Ä–∏–º–µ—Ä–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã.\n\n–û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ."

    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –≤ LLaMA API
    payload = {
        "model": "llama3",
        "prompt": prompt,
        "stream": False,
        "options": {
            "temperature": 0.7,
            "language": lang
        }
    }

    try:
        response = requests.post(OLLAMA_API_URL, json=payload, timeout=300)
        response_data = response.json()
        time.sleep(5)

        # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –µ–≥–æ
        llama_response = response_data.get("response", "–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞.")
        
        # –ï—Å–ª–∏ –≤ –æ—Ç–≤–µ—Ç–µ –µ—Å—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∞, –Ω–∞–ø—Ä–∏–º–µ—Ä, —Å—Å—ã–ª–∫–∏ –∏–ª–∏ —Ä–∞–∑–¥–µ–ª—ã, –º–æ–∂–Ω–æ –∏—Ö —Ä–∞–∑–¥–µ–ª–∏—Ç—å
        return llama_response

    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ Ollama: {e}"
    except requests.exceptions.ReadTimeout:
        return "–û—à–∏–±–∫–∞: –∑–∞–ø—Ä–æ—Å –∑–∞–Ω—è–ª —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
    except requests.exceptions.RequestException as e:
        return f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {e}"
    

# —Ñ–£–ù–ö–¶–ò–Ø –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
def cache_prompt_analysis(user_prompt, analysis_result):
    if not os.path.exists(CACHE_DIR):
        os.makedirs(CACHE_DIR)

    cache_filename = os.path.join(CACHE_DIR, f"{hash(user_prompt)}.json")

    # save results
    with open(cache_filename, 'w') as f:
        json.dump({
            'timestamp':time.time(),
            'analysis_result': analysis_result
        }, f)
def get_cached_analysis(user_prompt):
    cache_filename = os.path.join(CACHE_DIR, f"{hash(user_prompt)}.json")

    if os.path.exists(cache_filename):
        with open(cache_filename, 'r') as f:
            cached_data = json.load(f)

        if time.time() - cached_data['timestamp'] < CACHE_EXPIRATION_TIME:
            return cached_data['analysis_result']

    return None            
